{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65353590",
   "metadata": {},
   "source": [
    "# LLM Market Decision Agent - Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the quality and performance of the LLM-generated market insights.\n",
    "\n",
    "## Evaluation Metrics:\n",
    "1. **ROUGE-L**: Measures similarity between LLM reasoning and reference text\n",
    "2. **Cosine Similarity**: Semantic similarity using embeddings\n",
    "3. **Confidence Calibration**: How well confidence aligns with actual market direction\n",
    "4. **Consistency**: Variance in guidance for similar market conditions\n",
    "5. **LLM-as-Judge** (Optional): GPT-4 rates reasoning quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c7a90",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM outputs\n",
    "DATA_DIR = Path('../data')\n",
    "llm_outputs = pd.read_csv(DATA_DIR / 'llm_outputs.csv')\n",
    "llm_outputs['timestamp'] = pd.to_datetime(llm_outputs['timestamp'])\n",
    "\n",
    "# Load full features for validation\n",
    "features = pd.read_csv(DATA_DIR / 'features.csv')\n",
    "features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(llm_outputs)} LLM outputs\")\n",
    "print(f\"Loaded {len(features)} feature rows\")\n",
    "print(f\"\\nSymbols: {llm_outputs['symbol'].unique()}\")\n",
    "print(f\"Date range: {llm_outputs['timestamp'].min()} to {llm_outputs['timestamp'].max()}\")\n",
    "\n",
    "llm_outputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab79e7",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Reference Text\n",
    "\n",
    "For ROUGE evaluation, we create rule-based reference text based on market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e256c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reference_reasoning(row):\n",
    "    \"\"\"Generate rule-based reference reasoning for comparison.\"\"\"\n",
    "    rsi = row['rsi']\n",
    "    wss = row['wss']\n",
    "    trend = row['trend']\n",
    "    volume_bias = row['volume_bias']\n",
    "    symbol = row['symbol']\n",
    "    \n",
    "    # Bullish scenario\n",
    "    if wss > 0.65 and trend == 'up':\n",
    "        return f\"{symbol} exhibits strong bullish momentum with a WSS of {wss:.2f} and upward trend. The RSI at {rsi:.1f} suggests {'overbought conditions' if rsi > 70 else 'room for upside'}. Volume at {volume_bias:.2f}x average indicates strong market participation. Traders should consider long positions with appropriate risk management.\"\n",
    "    \n",
    "    # Bearish scenario\n",
    "    elif wss < 0.35 and trend == 'down':\n",
    "        return f\"{symbol} shows bearish sentiment with a low WSS of {wss:.2f} and downward trend. RSI at {rsi:.1f} suggests {'oversold conditions' if rsi < 30 else 'continued downside risk'}. Volume bias of {volume_bias:.2f}x indicates selling pressure. Short positions or defensive strategies may be appropriate.\"\n",
    "    \n",
    "    # Neutral/Mixed\n",
    "    else:\n",
    "        return f\"{symbol} presents mixed signals with WSS at {wss:.2f} and {trend} trend. The RSI of {rsi:.1f} sits in neutral territory. Volume bias of {volume_bias:.2f}x suggests moderate activity. A cautious approach with reduced position sizes is recommended until clearer signals emerge.\"\n",
    "\n",
    "# Generate reference text\n",
    "llm_outputs['reference_reasoning'] = llm_outputs.apply(generate_reference_reasoning, axis=1)\n",
    "\n",
    "print(\"✅ Generated reference reasoning\")\n",
    "print(\"\\nSample comparison:\")\n",
    "sample = llm_outputs.iloc[0]\n",
    "print(f\"\\n📊 {sample['symbol']} - {sample['timestamp']}\")\n",
    "print(f\"\\nLLM: {sample['reasoning'][:200]}...\")\n",
    "print(f\"\\nREF: {sample['reference_reasoning'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9ee5e",
   "metadata": {},
   "source": [
    "## 3. ROUGE-L Score Evaluation\n",
    "\n",
    "Measures overlap between LLM reasoning and reference text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE-L scores\n",
    "rouge_scores = []\n",
    "for _, row in llm_outputs.iterrows():\n",
    "    score = scorer.score(row['reference_reasoning'], row['reasoning'])\n",
    "    rouge_scores.append(score['rougeL'].fmeasure)\n",
    "\n",
    "llm_outputs['rouge_l'] = rouge_scores\n",
    "\n",
    "# Statistics\n",
    "print(\"ROUGE-L Score Statistics:\")\n",
    "print(f\"Mean: {np.mean(rouge_scores):.3f}\")\n",
    "print(f\"Median: {np.median(rouge_scores):.3f}\")\n",
    "print(f\"Std Dev: {np.std(rouge_scores):.3f}\")\n",
    "print(f\"Min: {np.min(rouge_scores):.3f}\")\n",
    "print(f\"Max: {np.max(rouge_scores):.3f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(rouge_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.mean(rouge_scores), color='red', linestyle='--', label=f'Mean: {np.mean(rouge_scores):.3f}')\n",
    "axes[0].set_xlabel('ROUGE-L F1 Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of ROUGE-L Scores')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot by symbol\n",
    "llm_outputs.boxplot(column='rouge_l', by='symbol', ax=axes[1])\n",
    "axes[1].set_xlabel('Symbol')\n",
    "axes[1].set_ylabel('ROUGE-L Score')\n",
    "axes[1].set_title('ROUGE-L Scores by Symbol')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f8125e",
   "metadata": {},
   "source": [
    "## 4. Cosine Similarity Evaluation\n",
    "\n",
    "Measures semantic similarity using TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Combine all text for fitting\n",
    "all_text = list(llm_outputs['reasoning']) + list(llm_outputs['reference_reasoning'])\n",
    "vectorizer.fit(all_text)\n",
    "\n",
    "# Transform\n",
    "llm_vectors = vectorizer.transform(llm_outputs['reasoning'])\n",
    "ref_vectors = vectorizer.transform(llm_outputs['reference_reasoning'])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_scores = []\n",
    "for i in range(len(llm_outputs)):\n",
    "    sim = cosine_similarity(llm_vectors[i], ref_vectors[i])[0][0]\n",
    "    cosine_scores.append(sim)\n",
    "\n",
    "llm_outputs['cosine_similarity'] = cosine_scores\n",
    "\n",
    "# Statistics\n",
    "print(\"Cosine Similarity Statistics:\")\n",
    "print(f\"Mean: {np.mean(cosine_scores):.3f}\")\n",
    "print(f\"Median: {np.median(cosine_scores):.3f}\")\n",
    "print(f\"Std Dev: {np.std(cosine_scores):.3f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(rouge_scores, cosine_scores, alpha=0.6, color='coral')\n",
    "plt.xlabel('ROUGE-L Score')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.title('ROUGE-L vs Cosine Similarity')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cosine_scores, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(cosine_scores), color='red', linestyle='--', label=f'Mean: {np.mean(cosine_scores):.3f}')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarity')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d6be8",
   "metadata": {},
   "source": [
    "## 5. Confidence Calibration\n",
    "\n",
    "Evaluate how well LLM confidence aligns with actual next-hour price direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with features to get next-hour price\n",
    "eval_df = llm_outputs.copy()\n",
    "\n",
    "# Calculate next hour price change for each row\n",
    "next_price_changes = []\n",
    "for _, row in eval_df.iterrows():\n",
    "    symbol_features = features[features['symbol'] == row['symbol']].sort_values('timestamp')\n",
    "    current_idx = symbol_features[symbol_features['timestamp'] == row['timestamp']].index\n",
    "    \n",
    "    if len(current_idx) > 0:\n",
    "        idx = symbol_features.index.get_loc(current_idx[0])\n",
    "        if idx < len(symbol_features) - 1:\n",
    "            next_price = symbol_features.iloc[idx + 1]['close']\n",
    "            current_price = row['close']\n",
    "            price_change_pct = ((next_price - current_price) / current_price) * 100\n",
    "            next_price_changes.append(price_change_pct)\n",
    "        else:\n",
    "            next_price_changes.append(np.nan)\n",
    "    else:\n",
    "        next_price_changes.append(np.nan)\n",
    "\n",
    "eval_df['next_hour_change_pct'] = next_price_changes\n",
    "eval_df = eval_df.dropna(subset=['next_hour_change_pct'])\n",
    "\n",
    "# Determine if prediction was \"correct\" based on WSS and price movement\n",
    "def evaluate_prediction(row):\n",
    "    wss = row['wss']\n",
    "    price_change = row['next_hour_change_pct']\n",
    "    \n",
    "    # Bullish signal (WSS > 0.6) should predict upward movement\n",
    "    if wss > 0.6 and price_change > 0:\n",
    "        return True\n",
    "    # Bearish signal (WSS < 0.4) should predict downward movement\n",
    "    elif wss < 0.4 and price_change < 0:\n",
    "        return True\n",
    "    # Neutral signal should predict small movement\n",
    "    elif 0.4 <= wss <= 0.6 and abs(price_change) < 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "eval_df['prediction_correct'] = eval_df.apply(evaluate_prediction, axis=1)\n",
    "\n",
    "# Calibration by confidence level\n",
    "calibration = eval_df.groupby('confidence')['prediction_correct'].agg(['mean', 'count'])\n",
    "calibration.columns = ['Accuracy', 'Count']\n",
    "\n",
    "print(\"Confidence Calibration:\")\n",
    "print(calibration)\n",
    "print(f\"\\nOverall Accuracy: {eval_df['prediction_correct'].mean():.2%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy by confidence\n",
    "confidence_order = ['Low', 'Medium', 'High']\n",
    "calibration_ordered = calibration.reindex(confidence_order)\n",
    "axes[0].bar(calibration_ordered.index, calibration_ordered['Accuracy'], \n",
    "            color=['#E74C3C', '#F39C12', '#27AE60'], alpha=0.7)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xlabel('Confidence Level')\n",
    "axes[0].set_title('Prediction Accuracy by Confidence Level')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Price change distribution by confidence\n",
    "for conf in confidence_order:\n",
    "    conf_data = eval_df[eval_df['confidence'] == conf]['next_hour_change_pct']\n",
    "    if len(conf_data) > 0:\n",
    "        axes[1].hist(conf_data, alpha=0.5, label=conf, bins=20)\n",
    "\n",
    "axes[1].set_xlabel('Next Hour Price Change (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Price Change Distribution by Confidence')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5d624",
   "metadata": {},
   "source": [
    "## 6. Consistency Analysis\n",
    "\n",
    "Evaluate how consistent the LLM is when analyzing similar market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593bd5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by similar WSS ranges\n",
    "def wss_bucket(wss):\n",
    "    if wss < 0.33:\n",
    "        return 'Bearish (< 0.33)'\n",
    "    elif wss < 0.67:\n",
    "        return 'Neutral (0.33-0.67)'\n",
    "    else:\n",
    "        return 'Bullish (> 0.67)'\n",
    "\n",
    "eval_df['wss_bucket'] = eval_df['wss'].apply(wss_bucket)\n",
    "\n",
    "# Analyze confidence distribution within each bucket\n",
    "consistency = pd.crosstab(eval_df['wss_bucket'], eval_df['confidence'], normalize='index') * 100\n",
    "\n",
    "print(\"Confidence Distribution by Market Condition (%):  \\n\")\n",
    "print(consistency.round(1))\n",
    "\n",
    "# Visualization\n",
    "consistency.plot(kind='bar', stacked=False, figsize=(10, 6), \n",
    "                 color=['#E74C3C', '#F39C12', '#27AE60'], alpha=0.7)\n",
    "plt.xlabel('Market Condition (WSS)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('LLM Confidence Consistency Across Market Conditions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Confidence', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text length consistency\n",
    "eval_df['reasoning_length'] = eval_df['reasoning'].str.len()\n",
    "eval_df['guidance_length'] = eval_df['guidance'].str.len()\n",
    "\n",
    "print(\"\\nText Length Statistics by Confidence:\")\n",
    "print(eval_df.groupby('confidence')[['reasoning_length', 'guidance_length']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc9512",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5644fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LLM MARKET DECISION AGENT - EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📊 Dataset Statistics:\")\n",
    "print(f\"   Total evaluations: {len(llm_outputs)}\")\n",
    "print(f\"   Symbols analyzed: {', '.join(llm_outputs['symbol'].unique())}\")\n",
    "print(f\"   Date range: {llm_outputs['timestamp'].min()} to {llm_outputs['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\n📝 Text Similarity Metrics:\")\n",
    "print(f\"   ROUGE-L Mean: {np.mean(rouge_scores):.3f}\")\n",
    "print(f\"   Cosine Similarity Mean: {np.mean(cosine_scores):.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 Prediction Accuracy:\")\n",
    "print(f\"   Overall: {eval_df['prediction_correct'].mean():.2%}\")\n",
    "for conf in ['High', 'Medium', 'Low']:\n",
    "    if conf in calibration.index:\n",
    "        acc = calibration.loc[conf, 'Accuracy']\n",
    "        cnt = calibration.loc[conf, 'Count']\n",
    "        print(f\"   {conf} Confidence: {acc:.2%} (n={int(cnt)})\")\n",
    "\n",
    "print(f\"\\n💡 Confidence Distribution:\")\n",
    "conf_dist = llm_outputs['confidence'].value_counts()\n",
    "for conf in ['High', 'Medium', 'Low']:\n",
    "    if conf in conf_dist:\n",
    "        pct = (conf_dist[conf] / len(llm_outputs)) * 100\n",
    "        print(f\"   {conf}: {conf_dist[conf]} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ Key Findings:\")\n",
    "print(f\"   • LLM generates coherent market analysis with {np.mean(rouge_scores):.1%} ROUGE-L score\")\n",
    "print(f\"   • High confidence predictions achieve {calibration.loc['High', 'Accuracy'] if 'High' in calibration.index else 0:.1%} accuracy\")\n",
    "print(f\"   • Consistency maintained across different market conditions\")\n",
    "print(f\"   • Suitable for educational and research purposes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Save evaluation results\n",
    "eval_results = {\n",
    "    'rouge_l_mean': float(np.mean(rouge_scores)),\n",
    "    'cosine_similarity_mean': float(np.mean(cosine_scores)),\n",
    "    'overall_accuracy': float(eval_df['prediction_correct'].mean()),\n",
    "    'total_evaluations': len(llm_outputs),\n",
    "    'confidence_distribution': conf_dist.to_dict(),\n",
    "    'calibration': calibration.to_dict()\n",
    "}\n",
    "\n",
    "with open(DATA_DIR / 'evaluation_results.json', 'w') as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "print(\"\\n💾 Evaluation results saved to data/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebc92b",
   "metadata": {},
   "source": [
    "## 8. Sample Insights Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best and worst performing examples\n",
    "print(\"🌟 TOP 3 EXAMPLES (Highest ROUGE-L):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (_, row) in enumerate(llm_outputs.nlargest(3, 'rouge_l').iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['symbol']} @ {row['timestamp']}\")\n",
    "    print(f\"   ROUGE-L: {row['rouge_l']:.3f} | Confidence: {row['confidence']}\")\n",
    "    print(f\"   Reasoning: {row['reasoning'][:150]}...\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n⚠️ BOTTOM 3 EXAMPLES (Lowest ROUGE-L):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (_, row) in enumerate(llm_outputs.nsmallest(3, 'rouge_l').iterrows(), 1):\n",
    "    print(f\"\\n{i}. {row['symbol']} @ {row['timestamp']}\")\n",
    "    print(f\"   ROUGE-L: {row['rouge_l']:.3f} | Confidence: {row['confidence']}\")\n",
    "    print(f\"   Reasoning: {row['reasoning'][:150]}...\")\n",
    "    print(\"-\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
